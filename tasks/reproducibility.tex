% template for a task
% each task should be added to exactly one workpackage
% in the workpackage task list
\begin{task}[
  title=Archiving software environments for reproducible computation,
  id=reproducibility,
  lead=XFEL,
  PM=42,
  wphases={1-36},
  partners={WTT,SRL,UPSUD,QS,EGI}
]

  Reproducible research can inspire greater confidence in scientific results,
  and make it easier for future research to build on those results.

  Reproducibility is seen as an essential pillar of scientific truth,
  nevertheless there is a real shortcoming of truly reproducible
  research in the areas of computational and data science. In part,
  this is a cultural matter. However, there is also a lack of
  computational e-infrastructure supporting reproducibility.
  \medskip

  Jupyter Notebooks combine explanation with code and output and are
  thus valuable tools for making scientific computing more
  reproducible. However, the code in a notebook invariably relies on
  external code: libraries and programs which are not saved as part of
  the notebook.

  This task concerns ways to record the versions of these tools in use, and to
  make them available for practical reproduction of the computation.

  Binder and its tool repo2docker are a first step in the right
  direction: given the description of a computational environment,
  they allow to create that computational environment as a docker
  container automatically on demand, which in turn allows to execute a
  given notebook within this container environment. By archiving the
  notebooks together with the environment specification, the container
  computation environment can be created on demand. We see a number of
  publications being complemented by such Git repositories that allow
  reproducing figures and results from papers by re-executing
  notebooks; often archiving these repositories via the Zenodo
  service (for example publication \cite{CortsOrtuo2018}) with github
  repository \cite{GithubRepoExampleCortes2018}).

  Container technologies, such as Docker, offer exciting possibilities
  for capturing a computational environment, but much of the
  development of these tools is focused on short-term operational
  uses, not long-term preservation.

  There are currently at least two limitations in the existing
  repo2docker approach:
\begin{enumerate}
\item the environment specifications need to be written carefully and
  need to explicitly define particular version numbers of operating
  systems, libraries, and software to be used in the
  environment. While there are no guidelines (yet) for best practice
  in writing such specifications, in principle users can do this
  correctly.
\item when repo2docker builds a container environment, it relies on
  the required software being available on the Internet: Commands that
  clone software from Github assume that the software is actually
  available on Github. If a relevant repository disappears (or Github
  disappears), it will be impossible to clone that software from
  there, and this will break the binder execution and thus
  reproducibility. Some environments are specified through
  Dockerfiles, and often start from an Ubuntu Linux distribution
  container, followed by an \texttt{apt-get update} command. This
  command will fail once the age of the specified distribution exceeds
  the support period, and similarly subsequent \texttt{apt-get
    install} commands will fail. As the Binder service matures, we
  start to see such failures occur.
\end{enumerate}

The task addresses these problems and includes the following activities:
\begin{compactitem}

\item Literature review and technology exploration: research Binder
  model and horizon scan for related technology to support long term
  reproducibility.

\item Establish and document best practice for Binder use: Create
  public guidelines for building containers for scientific computing
  purposes so that they remain useful in the longer term, building on
  existing technology such as repo2docker. ($\rightarrow$
  \localdelivref{binder-guidelines})

\item Facilitating reproducible creation and long-term archiving of
  container images for reproducibility: Develop new software to
  provide long-term executable computational environments that support
  the Binder model.

  There is a trade-off between preserving binary container images, and
  preserving the source code and instructions to build a container.
  Preserving sources is more transparent, and makes it easier to
  modify the code to explore a result, but without special care, the
  instructions may not continue to work in the future, or may not
  build an equivalent container.  We will explore both approaches,
  with a particular interest in how to make build instructions that
  can still work many years in the future.  ($\rightarrow$
  \localdelivref{jupyter-archive})

  It may be necessary for the build process to make use of alternative
  sources for source code and packages to install, such as snapshots
  of github repositories that are available on Zenodo, or dedicated
  software archive servers that provide selected pieces of software
  that are required to build particular containers.

  The developed software will be integrated into the EOSC Binder
  (\taskref{eosc}{eu-binder}).
  \end{compactitem}


%\begin{compactitem}
%  \item
%    (\localdelivref{deliv-id})
%  \end{compactitem}

This techology will be co-developed with a real scientific use case at
European XFEL (see \taskref{applications}{reproducibility-xfel} in
\WPref{applications}).

  We note that there is a wide variety of use cases for this kind of
  technology and advances towards long term reproducibility, which we
  expect to grow in importance over time and with the increasing
  acceptance of open science: journals do increasingly (and rightly
  so) request from authors that they can reproduce their studies, or
  even submit corresponding code with the submission of the papers. As
  Binder and notebooks are a popular way of achieving this, the long
  term executability becomes a challenge. The same is true for
  universities and other academic institutions that take FAIR data
  seriously, and want to support the reproducibility and re-usability
  aspect of data comprehensively through providing data analysis
  software that allows access to the meaning of the data.
\end{task}
